"""Example demonstrating Agent-S with Qwen3-32B and UI-TARS-1.5-7B.

This snippet shows how to configure Agent-S2 to use two remote vLLM
endpoints: one hosting Qwen3-32B for language generation and another
serving UI-TARS-1.5-7B for visual grounding. Replace the dummy URLs
with the addresses of your own deployments.

The visual grounding engine predicts coordinates from screenshots using
UI-TARS and guides the actions generated by the Qwen model.

Note:
    When running on Linux, ``pyautogui`` requires access to an X11
    server. Ensure that the ``DISPLAY`` environment variable points to
    your active session and that the Xauthority cookie (usually
    ``~/.Xauthority`` or ``$XAUTHORITY``) is readable by the process.
"""

import platform

import pyautogui

from gui_agents.s2.agents.agent_s import AgentS2
from gui_agents.s2.agents.grounding import OSWorldACI, scale_screen_dimensions
from gui_agents.s2.cli_app import run_agent

# ---------------------------------------------------------------------------
# Configuration of the main LLM (Qwen3-32B) served by a remote vLLM instance
# ---------------------------------------------------------------------------
# The engine type "vllm" tells Agent-S to call the specified base_url using
# the vLLM-compatible OpenAI API. Adjust the URL to match your deployment.
engine_params = {
    "engine_type": "vllm",
    "model": "Qwen3-32B",
    "base_url": "http://192.168.1.112:8002/v1",  # Qwen vLLM endpoint
    "api_key": "EMPTY",
}
# ---------------------------------------------------------------------------
# Configuration of the grounding model (UI-TARS-1.5-7B) served by vLLM
# ---------------------------------------------------------------------------
# UI-TARS automatically rescales images to 1000x1000. We pass those values so
# Agent-S can correctly transform the predicted coordinates back to the screen
# resolution.
engine_params_for_grounding = {
    "engine_type": "vllm",  # Using the same vLLM API as Qwen
    "model": "UI-TARS-1.5-7B",
    "base_url": "http://192.168.1.112:8001/v1",  # UI-TARS vLLM endpoint
    "api_key": "EMPTY",
    "grounding_width": 1000,
    "grounding_height": 1000,
}

# Obtain current screen resolution and compute a safe size for screenshots.
screen_width, screen_height = pyautogui.size()
scaled_width, scaled_height = scale_screen_dimensions(
    screen_width, screen_height, max_dim_size=2400
)

# ---------------------------------------------------------------------------
# Initialize the grounding agent. It uses Qwen for text-based commands that
# require OCR grounding and UI-TARS for coordinate generation.
# ---------------------------------------------------------------------------
current_platform = platform.system().lower()

grounding_agent = OSWorldACI(
    platform=current_platform,
    engine_params_for_generation=engine_params,
    engine_params_for_grounding=engine_params_for_grounding,
    width=screen_width,
    height=screen_height,
)

# ---------------------------------------------------------------------------
# Build the full AgentS2 instance. The search engine is disabled for clarity,
# but in a real setup you can enable Perplexica or another search backend.
# ---------------------------------------------------------------------------
agent = AgentS2(
    engine_params=engine_params,
    grounding_agent=grounding_agent,
    platform=current_platform,
    action_space="pyautogui",
    observation_type="mixed",
    search_engine=None,
)

# Example instruction that requires both generation and visual grounding.
# UI-TARS helps translate the Qwen-generated references into coordinates.
query = "Open a text editor and type 'Hello from Agent-S!'"

agent.reset()

# Run the agent on the host machine. This will continually query Qwen for
# actions, ground them with UI-TARS and execute them via pyautogui.
run_agent(agent, query, scaled_width, scaled_height)
